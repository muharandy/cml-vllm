{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "441bf1d4-fc4c-47d8-a082-5d730d54cce3",
   "metadata": {},
   "source": [
    "# Deploying LLM Model Serving using vLLM and FastAPI on CML\n",
    "\n",
    "In this tutorial we will be deploying LLM model serving on CML. Before we start, make sure you have the following:\n",
    "* Access to CML (either on CDP Public Cloud or PVC DS)\n",
    "* CML GPU Worker nodes\n",
    "    * Minimum 16GB VRAM (24GB or bigger is better to run bigger models)\n",
    "    * Minimum compute capability 7.0 (8.0 or higher is better to run model with bfloat16)\n",
    "* CML 2023.05 NVIDIA GPI runtimes with Python 3.10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8a4114-5a54-4f76-a0a3-33dc2715261c",
   "metadata": {},
   "source": [
    "## Install setuptools\n",
    "\n",
    "Do this first and then restart the kernel or restart your session just to make sure. We need to use a lower version of `setuptools` because CML default 2023.05 CUDA runtime has some issues during the installation for some of the python packages required for LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2a3c032-e8e7-4d2d-8456-f48a20519b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --q setuptools==59.8.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5c011f-3587-470e-9915-8b15009fd510",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Install vLLM\n",
    "\n",
    "VLLM allows faster inference using PagedAttention. More details can be found in https://vllm.ai\n",
    "\n",
    "We will be using the REST API server example given in the vllm package which uses FastAPI and adapted from Fastchat OpenAI REST API server implementation. The difference between the implementation of REST API server in VLLM example with the one in Fastchat is the VLLM example is simplified to use only one serving component. The Fastchat implementation requires a controller, at least one REST server, and multiple workers. \n",
    "\n",
    "Depending on the requirement, the Fastchat implementation can be used or adapted into a more scalable solution. However for the purpose of this tutorial we will be using VLLM example implementation as it will only require us to deploy a single Application in CML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1833c33e-6f82-4cb9-8625-dcb0abc75be6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --q vllm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c375bd6-e68a-4981-abf9-1c27ad5a607b",
   "metadata": {},
   "source": [
    "## Download the model\n",
    "\n",
    "Refer to the requirements stated above, several things to note here are:\n",
    "* VRAM size\n",
    "* Compute Capability\n",
    "\n",
    "For this example we will be using `lmsys/vicuna-7b-v1.3` since it is supported by vLLM, it can fit in a GPU with 16GB VRAM, and it can run on GPU with 7.0 compute capability. The only catch of running this model (or at least in my environment) is I cannot run it with the tokenizer that comes along with the model. I need to use `hf-internal-testing/llama-tokenizer` for it to work. For more details, refer to:\n",
    "* https://github.com/vllm-project/vllm/pull/284\n",
    "\n",
    "Optionally, you can download a different model and adjust the rest of the steps accordingly. Since we are using vLLM, just make sure that you are using one of the supported models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a375a9b9-ecd9-431d-afd1-e50b7b4159aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LFS and clone the model repo\n",
    "!git lfs install\n",
    "!git lfs clone https://huggingface.co/lmsys/vicuna-7b-v1.3\n",
    "\n",
    "# Move to models directory\n",
    "!mv vicuna-7b-v1.3/ models/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d2306d-3e42-4bc3-a633-7351fba1881f",
   "metadata": {},
   "source": [
    "### [Optional] Download Tokenizer\n",
    "Your milage may vary, but in my case I need use a different tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097d7e7f-0d68-426e-aad1-48b3612779be",
   "metadata": {},
   "outputs": [],
   "source": [
    "## download hf-internal-testing/llama-tokenizer\n",
    "!git lfs clone https://huggingface.co/hf-internal-testing/llama-tokenizer\n",
    "!mv llama-tokenizer/ models/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cdf2a6-498f-41c7-8b80-1d442b96b33c",
   "metadata": {},
   "source": [
    "## Prepare server script\n",
    "\n",
    "Create a directory called `server` and then create a file called `api_serve.py` with the following content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccdc1c1-31af-42b8-862a-ee92e2c6169b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m vllm.entrypoints.openai.api_server \\\n",
    "    --port $CDSW_APP_PORT \\\n",
    "    --host 127.0.0.1 \\\n",
    "    --model ./models/vicuna-7b-v1.3 \\\n",
    "    --tokenizer ./models/llama-tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266808ed-867e-4781-b58b-84cf685055dc",
   "metadata": {},
   "source": [
    "## Create Application\n",
    "\n",
    "Once the script is ready, we will create an application deployment in CML\n",
    "* Go to `Applications`\n",
    "* Click `New Application`\n",
    "* Fill in the `Name` and `Subdomain` as per your choosings\n",
    "* Select the script that you just created `server/api_serve.py`\n",
    "* Make sure to pick the NVIDIA runtime with Python 3.10\n",
    "* Select the resource profile as per the model requirement (suggested minimum: 4 vcores, 16GB, and 1GPU)\n",
    "* Once everything is filled up, hit `Create Application`\n",
    "\n",
    "You will wait for the application to be deployed and running, then click the application to open up a new tab in your browser.\n",
    "\n",
    "_Hint: At this moment CML will check for liveliness by constantly probing the REST server. This will create a swarm of HTTP 404 error due to non-existing path. You might need to disable logging once you make sure that the everything is running well_\n",
    "\n",
    "When you opened the application in a new tab in your browser, you will get something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168d06a0-2700-4589-8f3c-90b61f30bbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"detail\": \"Not Found\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db84b54-bc08-4613-a7f4-980a9c8e3dc4",
   "metadata": {},
   "source": [
    "That's actually a good sign. If you change the url by adding `v1/models` you will get something like this (depending on the model that you load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8c8816-d123-4311-8e48-e6e3b5da3ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"object\": \"list\",\n",
    "  \"data\": [\n",
    "    {\n",
    "      \"id\": \"./models/vicuna-7b-v1.3\",\n",
    "      \"object\": \"model\",\n",
    "      \"created\": 1690096055,\n",
    "      \"owned_by\": \"vllm\",\n",
    "      \"root\": \"./models/vicuna-7b-v1.3\",\n",
    "      \"parent\": null,\n",
    "      \"permission\": [\n",
    "        {\n",
    "          \"id\": \"modelperm-906c6225ade24cceb83e496c21a4e061\",\n",
    "          \"object\": \"model_permission\",\n",
    "          \"created\": 1690096055,\n",
    "          \"allow_create_engine\": false,\n",
    "          \"allow_sampling\": true,\n",
    "          \"allow_logprobs\": true,\n",
    "          \"allow_search_indices\": false,\n",
    "          \"allow_view\": true,\n",
    "          \"allow_fine_tuning\": false,\n",
    "          \"organization\": \"*\",\n",
    "          \"group\": null,\n",
    "          \"is_blocking\": false\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df3a912-bb89-421f-b7c8-54bc8165d6bc",
   "metadata": {},
   "source": [
    "You can also try accessing the REST API using `curl` following OpenAI REST API specs by changing the `v1/models` to `/v1/completions/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c20d212-a3a3-443f-9944-361a003fc2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl <YOUR_API_SERVER_URL>/v1/completions \\\n",
    "    -H \"Content-Type: application/json\" \\\n",
    "    -d '{\n",
    "        \"model\": \"./models/mpt-7b-instruct\",\n",
    "        \"prompt\": \"Tell me about Apache Iceberg\",\n",
    "        \"max_tokens\": 300,\n",
    "        \"temperature\": 0\n",
    "    }'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ce3fce-e91f-436f-ba2f-fcfa3a7b530b",
   "metadata": {},
   "source": [
    "### Configure REST Server URL as CML Environment Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9c804a-9e29-46d1-a48e-1624966a831a",
   "metadata": {},
   "source": [
    "If things are working correctly, copy our faux OpenAI base URL which is API Server URL deployed in CML up until `v1` omitting the `completions` or the `models`\n",
    "\n",
    "* Then go to `Project Settings` -> `Advanced`\n",
    "* Create a new variable as `LLM_API_SERVER_BASE` \n",
    "* Paste your base URL and hit `Submit`\n",
    "\n",
    "_Hint: You can also set this in the `Site Administration` level to make it globally accessible. You can also make the LLM path as environment variable. In our example we can make `./models/vicuna-7b-v1.3` as `LLM_LOADED`_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbe52d8-9b3e-4697-8f68-2a003afec1dc",
   "metadata": {},
   "source": [
    "## Accessing The REST API from Sessions\n",
    "\n",
    "Create a CPU session (no need GPU) then install the `openai` client package by running `pip install openai`. We are not using OpenAI service here. We are only using the client library and use our recently deployed REST API server instead\n",
    "\n",
    "Head on to examples directory for to see how we can make use of our recently deployed REST API. This list will keep growing as I build more examples:\n",
    "* `Simple.ipynb` For a simple example of completions API\n",
    "* `Simple-streaming.ipynb` For a simple example but utilizing streaming token output instead of batch\n",
    "* `Simple-RAG.ipynb` [WIP] For an example of RAG text generation from a supplied document context\n",
    "* `Pandasai.ipynb` [WIP] For an example on how to use PandasAI (requires Starcoder model to be deployed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e8695d-2a15-4243-bf33-9cffe2b6aa56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
